{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625dc738-a3b0-4edc-be7c-59c0ea834299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[0., 1.]])\n",
      "Post activation: 0.0\n",
      "Pre activation: -0.60\n",
      "Input:  tensor([[1., 0.]])\n",
      "Post activation: 0.0\n",
      "Pre activation: -0.40\n",
      "Input:  tensor([[0., 0.]])\n",
      "Post activation: 0.0\n",
      "Pre activation: -1.00\n",
      "Input:  tensor([[1., 1.]])\n",
      "Post activation: 1.0\n",
      "Pre activation: 0.00\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Simple neuron implementation using PyTorch\n",
    "# Pytorch uses tensors, which can be regarded\n",
    "# as generalizations of arrays that support differentiation\n",
    "# and other numerical and computational operations.\n",
    "#\n",
    "# Author: Prof. FabrÃ­cio Galende Marques de Carvalho\n",
    "#\n",
    "\n",
    "\n",
    "import neuron_model_pytorch as nmpt\n",
    "import torch\n",
    "\n",
    "def activation(x):\n",
    "    return 1. if x>=0 else 0.0\n",
    "\n",
    "# Float numbers shall include ., otherwise they will be treaded\n",
    "# as integers. \n",
    "neuron = nmpt.Neuron([0.6, 0.4], [-1.], activation)\n",
    "\n",
    "# Single line, two collumn vector. \n",
    "# When using Pytorch, each row vector is considered a sample,\n",
    "# while each collumn is considered a single feature.\n",
    "# So, the Pytorch model for activation potential (i.e.,\n",
    "# pre activation, is given by por W.X^T\n",
    "x = torch.tensor([[0.0, 1.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n",
    "x = torch.tensor([[1.0, 0.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n",
    "x = torch.tensor([[0.0, 0.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n",
    "x = torch.tensor([[1.0, 1.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0250e8-b550-4b2f-b416-c0a1df6490f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[1., 1.]])\n",
      "Post activation: tensor([[0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "Pre activation: 0.00\n",
      "Input:  tensor([[1., 0.]])\n",
      "Post activation: tensor([[0.4013]], grad_fn=<SigmoidBackward0>)\n",
      "Pre activation: -0.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we change the activation function to the built-in pytorch\n",
    "# sigmoid function\n",
    "neuron.act_fcn = torch.sigmoid\n",
    "x = torch.tensor([[1.0, 1.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n",
    "x = torch.tensor([[1.0, 0.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bd6a2d-09da-4c3c-8c02-dbd39a9b343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[1., 1.]])\n",
      "Post activation: tensor([[0.]], grad_fn=<ReluBackward0>)\n",
      "Pre activation: 0.00\n",
      "Input:  tensor([[1., 0.]])\n",
      "Post activation: tensor([[0.]], grad_fn=<ReluBackward0>)\n",
      "Pre activation: -0.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we change to ReLU\n",
    "neuron.act_fcn = torch.nn.ReLU()\n",
    "x = torch.tensor([[1.0, 1.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n",
    "\n",
    "x = torch.tensor([[1.0, 0.0]])  \n",
    "print(\"Input: \", x)\n",
    "print(\"Post activation:\", neuron.output(x))\n",
    "print(\"Pre activation: %2.2f\" %(neuron.pre_act))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b99bf-6969-4f5c-8352-bb17eedcc5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neurocomputing)",
   "language": "python",
   "name": "neurocomputing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
